{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12b161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d11ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "dataset_path = Path.cwd().parent / \"data/tokenized_squad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa0c97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32630a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = load_from_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a72e566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Load SQuAD dataset\n",
    "dataset = load_dataset(\"squad\", split=\"validation\")\n",
    "\n",
    "# Initialize QA pipeline\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1,  # Use GPU if available (-1 for CPU)\n",
    ")\n",
    "\n",
    "# Prepare evaluation\n",
    "squad_metric = evaluate.load(\"squad\")\n",
    "predictions = []\n",
    "references = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226b00d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(total=10570)\n",
    "\n",
    "for sample in dataset:\n",
    "    pbar.update(1)\n",
    "    # Generate prediction\n",
    "    prediction = qa_pipeline(\n",
    "        question=sample[\"question\"],\n",
    "        context=sample[\"context\"],\n",
    "        max_seq_len=384,\n",
    "        doc_stride=128,\n",
    "        handle_impossible_answer=False,\n",
    "    )\n",
    "\n",
    "    # Format prediction\n",
    "    formatted_prediction = {\n",
    "        \"id\": sample[\"id\"],\n",
    "        \"prediction_text\": prediction[\"answer\"],\n",
    "    }\n",
    "\n",
    "    # Format reference\n",
    "    formatted_reference = {\n",
    "        \"id\": sample[\"id\"],\n",
    "        \"answers\": sample[\"answers\"],\n",
    "    }\n",
    "\n",
    "    predictions.append(formatted_prediction)\n",
    "    references.append(formatted_reference)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c560fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = squad_metric.compute(predictions=predictions, references=references)\n",
    "print(f\"F1 Score: {results['f1']:.2f}\")\n",
    "print(f\"Exact Match: {results['exact_match']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
