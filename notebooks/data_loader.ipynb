{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f108beab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import BertForQuestionAnswering, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0f28831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0c4bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# squad v1\n",
    "dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da830087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Tokenize both questions and contexts\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        # Only truncate the context (second sequence)\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,  # Maximum length for BERT\n",
    "        stride=128,  # Overlap between chunks when splitting long contexts\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features,\n",
    "    # we need a map from feature to example\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Initialize lists to store the labels\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # Get the corresponding example index\n",
    "        sample_index = sample_mapping[i]\n",
    "\n",
    "        # Get the answer\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        start_char = answers[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "        # Find the start and end token positions\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        context_start = 0\n",
    "        while sequence_ids[context_start] != 1:\n",
    "            context_start += 1\n",
    "        context_end = len(sequence_ids) - 1\n",
    "        while sequence_ids[context_end] != 1:\n",
    "            context_end -= 1\n",
    "\n",
    "        # If the answer is out of the span, set to the CLS token\n",
    "        if (start_char < offsets[context_start][0]) or (\n",
    "            end_char > offsets[context_end][1]\n",
    "        ):\n",
    "            tokenized_examples[\"start_positions\"].append(0)\n",
    "            tokenized_examples[\"end_positions\"].append(0)\n",
    "        else:\n",
    "            # Find the start and end tokens\n",
    "            start_token = context_start\n",
    "            while (start_token <= context_end) and (\n",
    "                offsets[start_token][0] <= start_char\n",
    "            ):\n",
    "                start_token += 1\n",
    "            tokenized_examples[\"start_positions\"].append(start_token - 1)\n",
    "\n",
    "            end_token = context_end\n",
    "            while (\n",
    "                (end_token >= context_start) and (offsets[end_token][1])\n",
    "            ) >= end_char:\n",
    "                end_token -= 1\n",
    "            tokenized_examples[\"end_positions\"].append(end_token + 1)\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "\n",
    "# Apply the preprocessing to the dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,  # nbqa: E501\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a2c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7cf65da9db84f4ca90664c4a2aafc39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/88524 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e324121998434281ff412114fb855d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/10784 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset.save_to_disk(\"../data/tokenized_squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c197188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 88524\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 10784\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = load_from_disk(\"../data/tokenized_squad\")\n",
    "tokenized_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
